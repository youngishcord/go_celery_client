# Простой клиент Celery на Go

Используется только библиотека для связи с RabbitMQ `amqp091-go`, остальное реализовано с нуля.

Вопросы, на которые еще предстоит ответить:

- [ ] Как работать с цепочкой сообщений?
- [ ] Как реализовать ограничение по количеству чтений сообщений из очереди?
- [ ] Надо разобраться с `exchange` в rabbit.
- [ ] Нормальное хранение функций, та еще задача...
- [ ] Понять, как правильно возвращать ошибки.

В планах разработка:

- [ ] Нормальная структура клиента с интерфейсами, которые можно подменять для смены бекенда "RPC" на Redis и наоборот.
- [ ] Нормальный роутинг задач в разные горутины.
- [ ] Метаданные о задаче лежат в ее хедере в формате мапы. Их скорее всего надо распарсить вручную в структуру, чтобы было проще потом работать с этими данными. Но не займет ли это очень много времени? Как было бы оптимальнее?

В планах фичи:

- [ ] Получение сообщений от RabbitMQ.
- [ ] Ответ на сообщения в RabbitMQ через RPC.
- [ ] Ответ на сообщения через Redis.
- [ ] Работа с цепочкой задач.
- [ ] Реализовать heartbeat от воркера.
- [ ] Работа с chord
- [ ] Работа с group
- [ ] Работа с map
- [ ] Работа с chunk
- [ ] Уведомление о потере подключения к брокеру (и бекенду, наверное)

# Заметки по реализации

## Все работает через интерфейсы?

Получается так, что все компоненты работают через интерфейсы. Брокер и бекенд подменяемы, и задачи,
которые через них получаются, тоже должны работать через интерфейсы, поскольку имеют разный
набор вспомогательных параметров.

## Rpc backend в celery

При работе с rpc backend при отправке задачи в очередь publisher создает для
результата отдельную очередь, которая скорее всего исчезает после какого то времени,
но я не могу это проверить, поскольку дефолтное значение очень велико. Можно отдельно настроить
время жизни очереди, но это делается в Celery. Сейчас основная задача, поработать с Redis.

## Rabbit сам определяет слушателя, которому поступает задача

По всей видимости RabbitMQ сам определяет получателя сообщения по принципу roundrobin. Но я точно
читал, что каждый воркер может резервировать на себя только определенное количество задач, а
остальные остаются в свободном пуле. Надо это проверить.

Да каждый воркер имеет у себя параметр для резервирования количества задач, и этот параметр относится к
каналу подключения консюмера. Тоесть его можно настроить для каждого воркера отдельно.

Ниже приведен пример настройки, при котором резервируется 2 сообщения напрямую для Rabbit в Go

```Go
err = ch.Qos(
    2,     // prefetch count
    0,     // prefetch size (0 means unlimited)
    false, // global (false = per consumer, true = per channel)
)
```

## Переотправка сообщения

Если по какой то причине необходимо вернуть сообщение в пул очереди, можно использовать метод `Nack`, который
вернет задачу в очередь нетронутой. Это удобно, если у воркера, например, нет функции для исполнения полученной задачи.

```Go
if header.Task != "test" {
    err = message.Nack(false, true)
    if err != nil {
        log.Fatalln(err)
    } else {
        fmt.Println("task nack")
    }
}
```

## Тесты на работоспособность цепочек

Воркер должен реализовывать передачу задачи по цепочке, что накладывает ответственность по работе с обработкой
вложенных данных о цепочке. Воркер должен передавать результат корректно в зависимости от полученных параметров
и встраивать их внутрь args и kwargs. Нужно посмотреть, как именно это работает и реализовать подобное поведение.

При передаче результата по цепочке результат является первым параметром и должен передаваться в массив
на первую позицию.

Это происходит в воркере или в клиенте???

В Celery возврат args и kwargs в

## Подтверждение обработки или отмена задачи

### RabbitMQ

В стандарте у каждой базовой задачи, получаемой от RabbitMQ при использовании
библиотеки в структуре хранится канал подключения, из которого было получено
данное сообщение. Канал реализует интерфейс для подтверждения или отмены задач.
При этом, нет необходимости таскать канал за задачей, если я хочу сделать свой
универсальный тип и отвязаться от конкретной реализации. Самое важное, знать
изначальный канал и `DeliveryTag`, принадлежавший задаче.
Тогда можно спокойно, используя подключение, подтвердить задачу, например так:

```Go
conn, _ := amqp.Dial("amqp://guest:guest@localhost:5545/")
ch, _ := conn.Channel()
err = ch.Qos(
    // prefetch count Этот параметр не даст мне получать больше задач, чем сейчас выполняются,
    // следовательно, данный параметр должен быть настроен по количеству обработчиков
    2,
    0,     // prefetch size (0 means unlimited)
    false, // global (false = per consumer, true = per channel)
)

//pub, err := conn.Channel()
//if err != nil {
//	panic(err)
//}

msgs, err := ch.Consume(
    "qwer",
    fmt.Sprintf("consumer_"),
    false,
    false,
    false,
    false,
    nil,
)
if err != nil {
    panic(err)
}
msg := <-msgs
fmt.Println(string(msg.Body))

time.Sleep(10 * time.Second)

err = ch.Ack(msg.DeliveryTag, false) // Ура так работает!
if err != nil {
    panic(err)
}

//err = pub.Ack(msg.DeliveryTag, false) // Если канал не тот, из которого
//if err != nil {                       // пришло сообщение, то не работает!
//	panic(err)
//}
```

#### Ограничения на количество сообщений в RabbitMQ

Если ограничить количество выделяемых на подключение незавершенных задач, то rabbit
не будет выделять задачи на данное подключение, следовательно, задача не будет
зависать в неподтвержденном состоянии в канале, ожидая обработки. Главное, чтобы количество
выделяемых задач совпадало с количеством обработчиков!

## Обработка ошибок

### Redis

В редисе ошибки возвращаются в результате и в traceback вкладывается стек вызова в виде большой строки.
Надо придумать фабрику ошибок.

Пример формата ошибки.

```json
{
  "children": [],
  "date_done": "2025-09-22T19:56:50.199924+00:00",
  "result": {
    "exc_type": "ValueError",
    "exc_message": ["custom"],
    "exc_module": "builtins"
  },
  "status": "FAILURE",
  "task_id": "8a44f616-30bd-4330-b0f1-bb5d13c3c4df",
  "traceback": "Traceback (most recent call last):\n  File \"C:\\Users\\kulik\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\celery\\app\\trace.py\", line 453, in trace_task\n    R = retval = fun(*args, **kwargs)\n                 ~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\kulik\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\celery\\app\\trace.py\", line 736, in __protected_call__\n    return self.run(*args, **kwargs)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\kulik\\OneDrive\\Desktop\\programming\\golang\\go_celery_client\\py\\main.py\", line 27, in add\n    raise ValueError(\"custom\")\nValueError: custom\n"
}
```

### Rabbit

В Rabbit результат задачи с ошибкой идентичен, можно попробовать сделать единую фабрику для генерации результата.
Странненько, что результаты разные, а ошибки идентичны.

```json
{
  "children": [],
  "result": {
    "exc_type": "ValueError",
    "exc_message": ["custom"],
    "exc_module": "builtins"
  },
  "status": "FAILURE",
  "task_id": "a4614b17-9294-4472-af3d-6725b949772c",
  "traceback": "Traceback (most recent call last):\n  File \"C:\\Users\\kulik\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\celery\\app\\trace.py\", line 453, in trace_task\n    R = retval = fun(*args, **kwargs)\n                 ~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\kulik\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\celery\\app\\trace.py\", line 736, in __protected_call__\n    return self.run(*args, **kwargs)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\kulik\\OneDrive\\Desktop\\programming\\golang\\go_celery_client\\py\\main.py\", line 27, in add\n    raise ValueError(\"custom\")\nValueError: custom\n"
}
```

# Ссылки

- Протокол сообщений Celery https://docs.celeryq.dev/en/latest/internals/protocol.html
